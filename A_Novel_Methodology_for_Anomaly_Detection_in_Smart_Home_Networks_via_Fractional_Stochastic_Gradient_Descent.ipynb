{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "0QpUQRo0FZTg",
        "outputId": "9a566582-9314-4953-f926-88b92ef48ecf"
      },
      "outputs": [],
      "source": [
        "!pip install d2l"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MgRGtNzdg6xQ",
        "outputId": "6b43a132-bb22-484b-b252-9ac3e9c0a548"
      },
      "outputs": [],
      "source": [
        "\n",
        "import math\n",
        "import random\n",
        "import torch\n",
        "import time\n",
        "from torch import nn\n",
        "from torch.optim.optimizer import Optimizer\n",
        "from sklearn.metrics import r2_score\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from d2l import torch as d2l\n",
        "from collections import deque\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Read data\n",
        "df_spark = pd.read_csv('df_spark.csv')\n",
        "df_spark = df_spark.drop(columns=\"Unnamed: 0\")\n",
        "\n",
        "y = df_spark.iloc[:, 0].values\n",
        "X = df_spark.iloc[:, 1:].values\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.10, random_state=1)\n",
        "\n",
        "# Standardize data\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Define the custom optimizer\n",
        "class FCSGD_G_L(Optimizer):\n",
        "    def __init__(self, params, lr=1e-1, weight_decay=1e-8, r=1.1):\n",
        "        if not 0.0 <= lr:\n",
        "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
        "        if not 0.0 <= weight_decay:\n",
        "            raise ValueError(\"Invalid weight_decay value: {}\".format(weight_decay))\n",
        "        if not -2.0 <= r:\n",
        "            raise ValueError(\"Invalid r value: {}\".format(r))\n",
        "        defaults = dict(lr=lr, weight_decay=weight_decay, r=r)\n",
        "        super(FCSGD_G_L, self).__init__(params, defaults)\n",
        "\n",
        "    def __setstate__(self, state):\n",
        "        super(FCSGD_G_L, self).__setstate__(state)\n",
        "\n",
        "    def step(self, closure=None):\n",
        "        loss = None\n",
        "        if closure is not None:\n",
        "            loss = closure()\n",
        "\n",
        "        for group in self.param_groups:\n",
        "            for p in group['params']:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "                grad = p.grad.data\n",
        "                if grad.is_sparse:\n",
        "                    raise RuntimeError('FCSGD_G_L does not support sparse gradients')\n",
        "\n",
        "                state = self.state[p]\n",
        "\n",
        "                # State initialization\n",
        "                if len(state) == 0:\n",
        "                    state['step'] = 0\n",
        "                    state['present_grad'] = torch.zeros_like(p.data)\n",
        "                    # Previous gradient\n",
        "                    state['previous_grad_1'] = torch.zeros_like(p.data)\n",
        "                    state['previous_grad_2'] = torch.zeros_like(p.data)\n",
        "                    state['previous_grad_3'] = torch.zeros_like(p.data)\n",
        "                    state['previous_grad_4'] = torch.zeros_like(p.data)\n",
        "                    state['previous_grad_5'] = torch.zeros_like(p.data)\n",
        "                    state['previous_grad_6'] = torch.zeros_like(p.data)\n",
        "                    state['previous_grad_7'] = torch.zeros_like(p.data)\n",
        "                    state['previous_grad_8'] = torch.zeros_like(p.data)\n",
        "                    state['previous_grad_9'] = torch.zeros_like(p.data)\n",
        "                    state['previous_grad_10'] = torch.zeros_like(p.data)\n",
        "                    # Fractional order\n",
        "                    r = group['r']\n",
        "                    # Coefficients by a novel G-L\n",
        "                    state['w0'] = 1\n",
        "                    state['w1'] = (1 - (r + 1) / 2) * state['w0']\n",
        "                    state['w2'] = (1 - (r + 1) / 3) * state['w1']\n",
        "                    state['w3'] = (1 - (r + 1) / 4) * state['w2']\n",
        "                    state['w4'] = (1 - (r + 1) / 5) * state['w3']\n",
        "                    state['w5'] = (1 - (r + 1) / 6) * state['w4']\n",
        "                    state['w6'] = (1 - (r + 1) / 7) * state['w5']\n",
        "                    state['w7'] = (1 - (r + 1) / 8) * state['w6']\n",
        "                    state['w8'] = (1 - (r + 1) / 9) * state['w7']\n",
        "                    state['w9'] = (1 - (r + 1) / 10) * state['w8']\n",
        "                    state['w10'] = (1 - (r + 1) / 11) * state['w9']\n",
        "\n",
        "                fractional_order_grad = state['present_grad']\n",
        "\n",
        "                a = [1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
        "                random.shuffle(a)\n",
        "\n",
        "                fractional_order_grad.add_(1, grad.clone())\\\n",
        "                    .add_(a[0] * state['w1'], state['previous_grad_1'])\\\n",
        "                    .add_(a[1] * state['w2'], state['previous_grad_2'])\\\n",
        "                    .add_(a[2] * state['w3'], state['previous_grad_3'])\\\n",
        "                    .add_(a[3] * state['w4'], state['previous_grad_4'])\\\n",
        "                    .add_(a[4] * state['w5'], state['previous_grad_5'])\\\n",
        "                    .add_(a[5] * state['w6'], state['previous_grad_6'])\\\n",
        "                    .add_(a[6] * state['w7'], state['previous_grad_7'])\\\n",
        "                    .add_(a[7] * state['w8'], state['previous_grad_8'])\\\n",
        "                    .add_(a[8] * state['w9'], state['previous_grad_9'])\\\n",
        "                    .add_(a[9] * state['w10'], state['previous_grad_10'])\n",
        "\n",
        "                state['previous_grad_10'] = state['previous_grad_9']\n",
        "                state['previous_grad_9'] = state['previous_grad_8']\n",
        "                state['previous_grad_8'] = state['previous_grad_7']\n",
        "                state['previous_grad_7'] = state['previous_grad_6']\n",
        "                state['previous_grad_6'] = state['previous_grad_5']\n",
        "                state['previous_grad_5'] = state['previous_grad_4']\n",
        "                state['previous_grad_4'] = state['previous_grad_3']\n",
        "                state['previous_grad_3'] = state['previous_grad_2']\n",
        "                state['previous_grad_2'] = state['previous_grad_1']\n",
        "                state['previous_grad_1'] = grad.clone()\n",
        "\n",
        "                state['step'] += 1\n",
        "\n",
        "                if group['weight_decay'] != 0:\n",
        "                    fractional_order_grad.add_(group['weight_decay'], p.data)\n",
        "\n",
        "                fractional_order_grad_1 = fractional_order_grad\n",
        "                state['present_grad'] = torch.zeros_like(p.data)\n",
        "                step_size = group['lr']\n",
        "                p.data.add_(-step_size, fractional_order_grad_1)\n",
        "\n",
        "        return loss\n",
        "\n",
        "# Define the PyTorch model\n",
        "class SimpleMLP(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(SimpleMLP, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.fc1(x)\n",
        "        out = self.relu(out)\n",
        "        out = self.fc2(out)\n",
        "        return out\n",
        "\n",
        "# Convert data to PyTorch tensors\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "# Create DataLoader\n",
        "train_dataset = torch.utils.data.TensorDataset(X_train_tensor, y_train_tensor)\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "# Initialize model, optimizer, and loss function\n",
        "input_size = X_train.shape[1]\n",
        "hidden_size = 50\n",
        "output_size = len(np.unique(y))\n",
        "model = SimpleMLP(input_size, hidden_size, output_size)\n",
        "\n",
        "optimizer = FCSGD_G_L(model.parameters(), lr=0.1)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    for batch_X, batch_y in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(batch_X)\n",
        "        loss = criterion(outputs, batch_y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# Evaluate the model\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    train_outputs = model(X_train_tensor)\n",
        "    test_outputs = model(X_test_tensor)\n",
        "    train_preds = torch.argmax(train_outputs, dim=1)\n",
        "    test_preds = torch.argmax(test_outputs, dim=1)\n",
        "    train_acc = (train_preds == y_train_tensor).float().mean()\n",
        "    test_acc = (test_preds == y_test_tensor).float().mean()\n",
        "\n",
        "print(f'Train Accuracy: {train_acc:.4f}, Test Accuracy: {test_acc:.4f}')\n",
        "\n",
        "# Classification report\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "y_train_pred = train_preds.numpy()\n",
        "y_test_pred = test_preds.numpy()\n",
        "target_names = ['Normal', 'DoSattack', 'scan', 'malitiousControl', 'malitiousOperation', 'spying', 'dataProbing', 'wrongSetUp']\n",
        "\n",
        "print(classification_report(y_train, y_train_pred, target_names=target_names))\n",
        "print(classification_report(y_test, y_test_pred, target_names=target_names))\n",
        "\n",
        "# Confusion matrix\n",
        "cnf_matrix = confusion_matrix(y_test, y_test_pred)\n",
        "print(cnf_matrix)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dqiTiM-XdxDp",
        "outputId": "a5101bdf-4fc3-4171-bb57-80b355758cc0"
      },
      "outputs": [],
      "source": [
        "#for time\n",
        "import math\n",
        "\n",
        "import random\n",
        "import torch\n",
        "import time\n",
        "from torch import nn\n",
        "from torch.optim import Adam, SGD\n",
        "from torch.optim.optimizer import Optimizer\n",
        "from sklearn.metrics import r2_score, classification_report, confusion_matrix\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from d2l import torch as d2l\n",
        "from collections import deque\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Read data\n",
        "df_spark = pd.read_csv('df_spark.csv')\n",
        "df_spark = df_spark.drop(columns=\"Unnamed: 0\")\n",
        "\n",
        "y = df_spark.iloc[:, 0].values\n",
        "X = df_spark.iloc[:, 1:].values\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=1)\n",
        "\n",
        "# Standardize data\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Define the custom optimizer\n",
        "class FCSGD_G_L(Optimizer):\n",
        "    def __init__(self, params, lr=1e-1, weight_decay=1e-8, r=0.8):\n",
        "        if not 0.0 <= lr:\n",
        "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
        "        if not 0.0 <= weight_decay:\n",
        "            raise ValueError(\"Invalid weight_decay value: {}\".format(weight_decay))\n",
        "        if not -2.0 <= r:\n",
        "            raise ValueError(\"Invalid r value: {}\".format(r))\n",
        "        defaults = dict(lr=lr, weight_decay=weight_decay, r=r)\n",
        "        super(FCSGD_G_L, self).__init__(params, defaults)\n",
        "\n",
        "    def __setstate__(self, state):\n",
        "        super(FCSGD_G_L, self).__setstate__(state)\n",
        "\n",
        "    def step(self, closure=None):\n",
        "        loss = None\n",
        "        if closure is not None:\n",
        "            loss = closure()\n",
        "\n",
        "        for group in self.param_groups:\n",
        "            for p in group['params']:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "                grad = p.grad.data\n",
        "                if grad.is_sparse:\n",
        "                    raise RuntimeError('FCSGD_G_L does not support sparse gradients')\n",
        "\n",
        "                state = self.state[p]\n",
        "\n",
        "                # State initialization\n",
        "                if len(state) == 0:\n",
        "                    state['step'] = 0\n",
        "                    state['present_grad'] = torch.zeros_like(p.data)\n",
        "                    # Previous gradient\n",
        "                    state['previous_grad_1'] = torch.zeros_like(p.data)\n",
        "                    state['previous_grad_2'] = torch.zeros_like(p.data)\n",
        "                    state['previous_grad_3'] = torch.zeros_like(p.data)\n",
        "                    state['previous_grad_4'] = torch.zeros_like(p.data)\n",
        "                    state['previous_grad_5'] = torch.zeros_like(p.data)\n",
        "                    state['previous_grad_6'] = torch.zeros_like(p.data)\n",
        "                    state['previous_grad_7'] = torch.zeros_like(p.data)\n",
        "                    state['previous_grad_8'] = torch.zeros_like(p.data)\n",
        "                    state['previous_grad_9'] = torch.zeros_like(p.data)\n",
        "                    state['previous_grad_10'] = torch.zeros_like(p.data)\n",
        "                    # Fractional order\n",
        "                    r = group['r']\n",
        "                    # Coefficients by a novel G-L\n",
        "                    state['w0'] = 1\n",
        "                    state['w1'] = (1 - (r + 1) / 2) * state['w0']\n",
        "                    state['w2'] = (1 - (r + 1) / 3) * state['w1']\n",
        "                    state['w3'] = (1 - (r + 1) / 4) * state['w2']\n",
        "                    state['w4'] = (1 - (r + 1) / 5) * state['w3']\n",
        "                    state['w5'] = (1 - (r + 1) / 6) * state['w4']\n",
        "                    state['w6'] = (1 - (r + 1) / 7) * state['w5']\n",
        "                    state['w7'] = (1 - (r + 1) / 8) * state['w6']\n",
        "                    state['w8'] = (1 - (r + 1) / 9) * state['w7']\n",
        "                    state['w9'] = (1 - (r + 1) / 10) * state['w8']\n",
        "                    state['w10'] = (1 - (r + 1) / 11) * state['w9']\n",
        "\n",
        "                fractional_order_grad = state['present_grad']\n",
        "\n",
        "                a = [1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
        "                random.shuffle(a)\n",
        "\n",
        "                fractional_order_grad.add_(1, grad.clone())\\\n",
        "                    .add_(a[0] * state['w1'], state['previous_grad_1'])\\\n",
        "                    .add_(a[1] * state['w2'], state['previous_grad_2'])\\\n",
        "                    .add_(a[2] * state['w3'], state['previous_grad_3'])\\\n",
        "                    .add_(a[3] * state['w4'], state['previous_grad_4'])\\\n",
        "                    .add_(a[4] * state['w5'], state['previous_grad_5'])\\\n",
        "                    .add_(a[5] * state['w6'], state['previous_grad_6'])\\\n",
        "                    .add_(a[6] * state['w7'], state['previous_grad_7'])\\\n",
        "                    .add_(a[7] * state['w8'], state['previous_grad_8'])\\\n",
        "                    .add_(a[8] * state['w9'], state['previous_grad_9'])\\\n",
        "                    .add_(a[9] * state['w10'], state['previous_grad_10'])\n",
        "\n",
        "                state['previous_grad_10'] = state['previous_grad_9']\n",
        "                state['previous_grad_9'] = state['previous_grad_8']\n",
        "                state['previous_grad_8'] = state['previous_grad_7']\n",
        "                state['previous_grad_7'] = state['previous_grad_6']\n",
        "                state['previous_grad_6'] = state['previous_grad_5']\n",
        "                state['previous_grad_5'] = state['previous_grad_4']\n",
        "                state['previous_grad_4'] = state['previous_grad_3']\n",
        "                state['previous_grad_3'] = state['previous_grad_2']\n",
        "                state['previous_grad_2'] = state['previous_grad_1']\n",
        "                state['previous_grad_1'] = grad.clone()\n",
        "\n",
        "                state['step'] += 1\n",
        "\n",
        "                if group['weight_decay'] != 0:\n",
        "                    fractional_order_grad.add_(group['weight_decay'], p.data)\n",
        "\n",
        "                fractional_order_grad_1 = fractional_order_grad\n",
        "                state['present_grad'] = torch.zeros_like(p.data)\n",
        "                step_size = group['lr']\n",
        "                p.data.add_(-step_size, fractional_order_grad_1)\n",
        "\n",
        "        return loss\n",
        "\n",
        "# Define the PyTorch model\n",
        "class SimpleMLP(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(SimpleMLP, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.fc1(x)\n",
        "        out = self.relu(out)\n",
        "        out = self.fc2(out)\n",
        "        return out\n",
        "\n",
        "# Convert data to PyTorch tensors\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "# Create DataLoader\n",
        "train_dataset = torch.utils.data.TensorDataset(X_train_tensor, y_train_tensor)\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "# Initialize model, optimizer, and loss function\n",
        "input_size = X_train.shape[1]\n",
        "hidden_size = 50\n",
        "output_size = len(np.unique(y))\n",
        "model = SimpleMLP(input_size, hidden_size, output_size)\n",
        "\n",
        "# Function to measure training time for an optimizer\n",
        "def measure_training_time(optimizer_class, **optimizer_kwargs):\n",
        "    model = SimpleMLP(input_size, hidden_size, output_size)\n",
        "    optimizer = optimizer_class(model.parameters(), **optimizer_kwargs)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    start_time = time.time()\n",
        "    for epoch in range(1):\n",
        "        model.train()\n",
        "        for batch_X, batch_y in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(batch_X)\n",
        "            loss = criterion(outputs, batch_y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "    end_time = time.time()\n",
        "    return end_time - start_time\n",
        "\n",
        "# Measure average training time per epoch\n",
        "num_repeats = 5\n",
        "\n",
        "fcsdg_l_times = [measure_training_time(FCSGD_G_L, lr=0.1) for _ in range(num_repeats)]\n",
        "adam_times = [measure_training_time(Adam, lr=0.001) for _ in range(num_repeats)]\n",
        "sgd_times = [measure_training_time(SGD, lr=0.01) for _ in range(num_repeats)]\n",
        "\n",
        "avg_fcsdg_l_time = np.mean(fcsdg_l_times)\n",
        "avg_adam_time = np.mean(adam_times)\n",
        "avg_sgd_time = np.mean(sgd_times)\n",
        "\n",
        "print(f'Average training time per epoch (FCSGD_G_L): {avg_fcsdg_l_time:.4f} seconds')\n",
        "print(f'Average training time per epoch (Adam): {avg_adam_time:.4f} seconds')\n",
        "print(f'Average training time per epoch (SGD): {avg_sgd_time:.4f} seconds')\n",
        "\n",
        "# Training loop with timing\n",
        "num_epochs = 20\n",
        "optimizer_fcsdg_l = FCSGD_G_L(model.parameters(), lr=0.1)\n",
        "optimizer_adam = Adam(model.parameters(), lr=0.001)\n",
        "optimizer_sgd = SGD(model.parameters(), lr=0.01)\n",
        "\n",
        "def train_model(optimizer, model, num_epochs):\n",
        "    model.train()\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    epoch_times = []\n",
        "    for epoch in range(num_epochs):\n",
        "        start_time = time.time()\n",
        "        for batch_X, batch_y in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(batch_X)\n",
        "            loss = criterion(outputs, batch_y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        end_time = time.time()\n",
        "        epoch_times.append(end_time - start_time)\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
        "    return epoch_times\n",
        "\n",
        "# Train models and measure times\n",
        "fcsdg_l_times = train_model(optimizer_fcsdg_l, model, num_epochs)\n",
        "adam_times = train_model(optimizer_adam, model, num_epochs)\n",
        "sgd_times = train_model(optimizer_sgd, model, num_epochs)\n",
        "\n",
        "# Average time per epoch\n",
        "avg_fcsdg_l_time = np.mean(fcsdg_l_times)\n",
        "avg_adam_time = np.mean(adam_times)\n",
        "avg_sgd_time = np.mean(sgd_times)\n",
        "\n",
        "print(f'Average training time per epoch (FCSGD_G_L): {avg_fcsdg_l_time:.4f} seconds')\n",
        "print(f'Average training time per epoch (Adam): {avg_adam_time:.4f} seconds')\n",
        "print(f'Average training time per epoch (SGD): {avg_sgd_time:.4f} seconds')\n",
        "\n",
        "# Evaluate the model\n",
        "def evaluate_model(model, X_train_tensor, y_train_tensor, X_test_tensor, y_test_tensor):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        train_outputs = model(X_train_tensor)\n",
        "        test_outputs = model(X_test_tensor)\n",
        "        train_preds = torch.argmax(train_outputs, dim=1)\n",
        "        test_preds = torch.argmax(test_outputs, dim=1)\n",
        "        train_acc = (train_preds == y_train_tensor).float().mean()\n",
        "        test_acc = (test_preds == y_test_tensor).float().mean()\n",
        "\n",
        "    print(f'Train Accuracy: {train_acc:.4f}, Test Accuracy: {test_acc:.4f}')\n",
        "    return train_preds, test_preds\n",
        "\n",
        "train_preds, test_preds = evaluate_model(model, X_train_tensor, y_train_tensor, X_test_tensor, y_test_tensor)\n",
        "\n",
        "# Classification report\n",
        "y_train_pred = train_preds.numpy()\n",
        "y_test_pred = test_preds.numpy()\n",
        "target_names = ['Normal', 'DoSattack', 'scan', 'malitiousControl', 'malitiousOperation', 'spying', 'dataProbing', 'wrongSetUp']\n",
        "\n",
        "print(classification_report(y_train, y_train_pred, target_names=target_names))\n",
        "print(classification_report(y_test, y_test_pred, target_names=target_names))\n",
        "\n",
        "# Confusion matrix\n",
        "cnf_matrix = confusion_matrix(y_test, y_test_pred)\n",
        "print(cnf_matrix)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eQLOXcZdeDs6"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
